---
title: "MasterThesis2018_TF_IDF"
author: "Sarasati Palawita"
date: "27 December 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidytext)
library(stringr)
library(e1071)
library(ggplot2)
library(tibble)
library(caret)
library(RTextTools)
library(readxl)
library(tidyr)
library(purrr) #To sample
library(lessR) #Merge function
```

Retrieve Data
```{r}
setwd("C:/Users/Sarasati Palawita/Documents/Personal/BIPM 2017/Master Thesis 2018/Data")
cleaned_data = read_excel("cleaned_data_no_duplicate.xlsx")
```

Genre Distribution 
```{r}
barplot(prop.table(table(cleaned_data$genre)),
        col = rainbow(2),
        ylim = c(0, 0.7),
        main = "Genre Distribution")

```

Data Partition
```{r}
cleaned_data$lyrics <- as.character(cleaned_data$lyrics)
cleaned_data = na.omit(cleaned_data)
cleaned_data <- as_tibble(cleaned_data)
class(cleaned_data)

data <- cleaned_data
data_tf <- cleaned_data %>%
  group_by(genre) %>% 
  nest() %>%            
  mutate(n = c(1250)) %>% 
  mutate(samp = map2(data, n, sample_n)) %>% 
  select(genre, samp) %>%
  unnest()

data_tf <- data_tf %>%
  mutate(ids = row_number())

data <- data_tf
training_data <- data_tf %>%
  group_by(genre) %>% 
  nest() %>%            
  mutate(n = c(875)) %>% 
  mutate(samp = map2(data, n, sample_n)) %>% 
  select(genre, samp) %>%
  unnest()

#Make a list of training data song
song_list <- training_data$song

test_data <- data_tf %>%
  filter(!data_tf$song %in% song_list)

train_source <- training_data
test_source <- test_data

```

Data Conditioning
```{r }
# function to expand contractions in an English-language source
fix.contractions <- function(doc) {
  # "won't" is a special case as it does not expand to "wo not"
  doc <- gsub("won't", "will not", doc)
  doc <- gsub("can't", "can not", doc)
  doc <- gsub("n't", " not", doc)
  doc <- gsub("'ll", " will", doc)
  doc <- gsub("'re", " are", doc)
  doc <- gsub("'ve", " have", doc)
  doc <- gsub("'m", " am", doc)
  doc <- gsub("'d", " would", doc)
  # 's could be 'is' or could be possessive: it has no expansion
  doc <- gsub("'s", "", doc)
  return(doc)
}
train_source$lyrics <- sapply(train_source$lyrics, fix.contractions)
test_source$lyrics <- sapply(test_source$lyrics, fix.contractions)
```

To get rid of special characters
```{r removeSpecialChars}
# Training data - function to remove special characters
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]", " ", x)
# remove special characters
train_source$lyrics <- sapply(train_source$lyrics, removeSpecialChars)
# convert everything to lower case
train_source$lyrics <- sapply(train_source$lyrics, tolower)

# remove special characters
test_source$lyrics <- sapply(test_source$lyrics, removeSpecialChars)
# convert everything to lower case
test_source$lyrics <- sapply(test_source$lyrics, tolower)
#lyrics <- lyrics[]

```

To get rid of undesirable words
```{r undesirable_words}
undesirable_words <- c("chorus", "lyrics", "intro", "clichÃfÂ©s", "alright",
                       "theres", "bridge", "fe0f",
                       "chorus", "verse", "[chorus]", "[verse]",
                       "2", "2x", "3x", "1x",
                       "4", "ooh", "uurh", "uuh", "pheromone", "poompoom", "3121", "aaaaarrgghh", "aaaarrggghhhh",
                       "matic", " ai ", " ca ", " la ", " na ", 
                       " da ", " uh ", " tin ", "  ll", "transcription", "YoncÃfÂ©", "BeyoncÃfÂ©")
```

Tokenization
```{r}
#1. Unigrams - Tokenization
train_sources_tidy_unigrams <- train_source %>%
  unnest_tokens(word, lyrics) %>%
  anti_join(stop_words) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 2) %>%
  count(song, genre, word, ids, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, genre, n) %>%
  arrange(desc(tf_idf))

test_sources_tidy_unigrams <- test_source %>%
  unnest_tokens(word, lyrics) %>%
  anti_join(stop_words) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 2) %>%
  count(song, genre, word, ids, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, genre, n) %>%
  arrange(desc(tf_idf))

#2. Get the song which has more than 99 words - Top 100 tf_idf value per song
top100 <- train_sources_tidy_unigrams %>%
    count(song, ids, genre, word, song, tf, idf, tf_idf) %>%
    group_by(song) %>%
    mutate(len_word = length(word)) %>%
    filter(len_word > 99) %>%
    arrange(desc(tf_idf))

test_top100 <- test_sources_tidy_unigrams %>%
    count(song, ids, genre, word, song, n, tf, idf, tf_idf) %>%
    group_by(song) %>%
    mutate(len_word = length(word)) %>%
    filter(len_word > 99) %>%
    arrange(desc(tf_idf))

#3. Retrieve the Top 100 tf_idf 
          
data <- top100
training_data <- top100 %>%
  count(song, ids, genre, word, tf, idf, tf_idf) %>%
  group_by(genre) %>% 
  arrange(desc(tf_idf)) %>%
  top_n(n = 100, wt = tf_idf) %>%
  nest() %>%            
  mutate(n = c(100)) %>% 
  mutate(samp = map2(data, n, sample_n)) %>% 
  select(genre, samp) %>%
  unnest()

data <- test_top100
test_data <- test_top100 %>%
  count(song, ids, genre, word, song, n, tf, idf, tf_idf) %>%
  group_by(genre) %>% 
  top_n(n = 100, wt = tf_idf) %>% 
  nest() %>%            
  mutate(n = c(45)) %>% 
  mutate(samp = map2(data, n, sample_n)) %>% 
  select(genre, samp) %>%
  unnest()

#4. Define the x and y for training and testing data
training = data.frame(x=training_data$tf_idf, y=as.factor(training_data$genre))
testing = data.frame(x=test_data$tf_idf, y=as.factor(test_data$genre))

#5. Run Cross-Validation
trctrl <- trainControl(method = "repeatedcv",
                       number = 10, 
                       repeats = 3, 
                       verboseIter = FALSE)

set.seed(123)

#6. Train the model
svm_tfidf_uni <- train(y ~., data = training, method = "rbf",
                 trainControl=trctrl,
                 preProcess = c("center", "scale"),
                 tuneLength = 10)
svm_tfidf_uni

#7. Predict the model with testing dataset
test_pred <- predict(svm_Linear, testing)

#8. Check the model accuracy with confusion matrix
cm <- confusionMatrix(test_pred, testing$y)
cm$overall
cm$table

#EXTRA
#Boxplot
tf_idf <- training_data$tf_idf
tf_idf_test <- test_data$tf_idf
lexical_diversity <- data.frame(tf_idf,tf_idf_test)
boxplot(tf_idf)

#tf_idf score
training_data %>% 
  ggplot() +
  geom_boxplot(aes(genre, tf_idf), fill = "steelblue", alpha = 0.1) +
  labs(y = "tf-idf unigram", x = "Music Genre", 
       title = "Length of tf-idf unigram", 
       subtitle = " ")+
  ylim(0,0.3 )

test_data %>% 
  ggplot() +
  geom_boxplot(aes(genre, tf_idf), fill = "steelblue", alpha = 0.7) +
  labs(y = "tf-idf unigram", x = "Music Genre", 
       title = "Length of tf-idf unigram", 
       subtitle = " ")+
  ylim(0, 600)

```

Bigram - Tokenization
```{r}
#1. Bigrams - Tokenization
train_sources_tidy_bigrams <- train_source %>%
  unnest_tokens(word, lyrics, token = "ngrams", n = 2) %>%
  anti_join(stop_words) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 3) %>%
  count(genre, word, song, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, genre, n) %>%
  arrange(desc(tf_idf))

test_sources_tidy_bigrams <- test_source %>%
  unnest_tokens(word, lyrics, token = "ngrams", n = 2) %>%
  anti_join(stop_words) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 3) %>%
  count(genre, word, song, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, genre, n) %>%
  arrange(desc(tf_idf))

#2. Get the song which has more than 99 words - Top 100 tf_idf value per song
top100_bi <- train_sources_tidy_bigrams %>%
    count(song, genre, word, song, tf, idf, tf_idf) %>%
    group_by(song) %>%
    mutate(len_word = length(word)) %>%
    filter(len_word > 99) %>%
    arrange(desc(tf_idf))

test_top100_bi <- test_sources_tidy_bigrams %>%
    count(song, genre, word, song, n, tf, idf, tf_idf) %>%
    group_by(song) %>%
    mutate(len_word = length(word)) %>%
    filter(len_word > 99) %>%
    arrange(desc(tf_idf))

#3. Retrieve the Top 100 tf_idf 
data <- train_sources_tidy_bigrams
training_data_bi <- train_sources_tidy_bigrams %>%
  group_by(genre) %>% 
  top_n(100, tf_idf) %>%
  nest() %>%            
  mutate(n = c(100)) %>% 
  mutate(samp = map2(data, n, sample_n)) %>% 
  select(genre, samp) %>%
  unnest()

data <- test_sources_tidy_bigrams
test_data_bi <- test_sources_tidy_bigrams %>%
  group_by(genre) %>% 
  top_n(100, tf_idf) %>%
  nest() %>%            
  mutate(n = c(100)) %>% 
  mutate(samp = map2(data, n, sample_n)) %>% 
  select(genre, samp) %>%
  unnest()

#4. Define the x and y for training and testing data
training_bi = data.frame(x=training_data_bi$tf_idf, y=as.factor(training_data_bi$genre))
testing_bi = data.frame(x=test_data_bi$tf_idf, y=as.factor(test_data_bi$genre))

#5. Run CrossValidation
trctrl <- trainControl(method = "repeatedcv", 
                       number = 10, 
                       repeats = 5, 
                       verboseIter = FALSE)

#6. Train the model
svm_tfidf_bi <- train(y~., data = training_bi, method = "svmLinear",
                 trainControl=trctrl,
                 preProcess = c("center", "scale"),
                 tuneLength = 10)
svm_tfidf_bi

#7. Predict the model with test dataset
test_pred_bi <- predict(svm_Linear_bi, testing_bi)

#8. Check the model accuracy with confusion matrix
cm <- confusionMatrix(test_pred_bi, testing_bi$y)
cm$overall
cm$table
```


Trigram - Tokenization
```{r}
#1. Bigrams - Tokenization
train_sources_tidy_trigrams <- train_source %>%
  unnest_tokens(word, lyrics, token = "ngrams", n = 2) %>%
  anti_join(stop_words) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 3) %>%
  count(genre, word, song, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, genre, n) %>%
  arrange(desc(tf_idf))

test_sources_tidy_trigrams <- test_source %>%
  unnest_tokens(word, lyrics, token = "ngrams", n = 2) %>%
  anti_join(stop_words) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 3) %>%
  count(genre, word, song, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, genre, n) %>%
  arrange(desc(tf_idf))

#2. Get the song which has more than 99 words - Top 100 tf_idf value per song
top100_tri <- train_sources_tidy_trigrams %>%
    count(song, genre, word, song, tf, idf, tf_idf) %>%
    group_by(song) %>%
    mutate(len_word = length(word)) %>%
    filter(len_word > 99) %>%
    arrange(desc(tf_idf))

test_top100_tri <- test_sources_tidy_trigrams %>%
    count(song, genre, word, song, n, tf, idf, tf_idf) %>%
    group_by(song) %>%
    mutate(len_word = length(word)) %>%
    filter(len_word > 99) %>%
    arrange(desc(tf_idf))

#3. Retrieve the Top 100 tf_idf 
data <- train_sources_tidy_trigrams
training_data_tri <- train_sources_tidy_trigrams %>%
  group_by(genre) %>% 
  top_n(100, tf_idf) %>%
  nest() %>%            
  mutate(n = c(100)) %>% 
  mutate(samp = map2(data, n, sample_n)) %>% 
  select(genre, samp) %>%
  unnest()

data <- test_sources_tidy_trigrams
test_data_tri <- test_sources_tidy_trigrams %>%
  group_by(genre) %>% 
  top_n(100, tf_idf) %>%
  nest() %>%            
  mutate(n = c(100)) %>% 
  mutate(samp = map2(data, n, sample_n)) %>% 
  select(genre, samp) %>%
  unnest()

#4. Define the x and y for training and testing data
#Below are the data for Top 100 value
training_tri = data.frame(x=training_data_tri$tf_idf, y=as.factor(training_data_tri$genre))
testing_tri = data.frame(x=test_data_tri$tf_idf, y=as.factor(test_data_tri$genre))

#5. Run CrossValidation
trctrl <- trainControl(method = "repeatedcv", 
                       number = 10, 
                       repeats = 5, 
                       verboseIter = FALSE)

#6. Train the model
svm_tfidf_tri <- train(y~., data = training_tri, method = "svmLinear",
                 trainControl=trctrl,
                 preProcess = c("center", "scale"),
                 tuneLength = 10)
svm_tfidf_tri

#7. Predict the model with test dataset
test_pred_bi <- predict(svm_Linear_bi, testing_bi)

#8. Check the model accuracy with confusion matrix
cm <- confusionMatrix(test_pred_tri, testing_tri$y)
cm$overall
cm$table
```