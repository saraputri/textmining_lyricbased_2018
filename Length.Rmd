---
title: "Length"
author: "Sarasati Palawita"
date: "4 January 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)  # to read Excel files
library(stringr)
library(writexl)
library(purrr)
library(dplyr)
library(tidytext)
library(stringr)
library(e1071)
library(ggplot2)
library(tibble)
library(cleanNLP)
library(reticulate)
library(caret)
library(RTextTools)
library(readxl)
library(stringr)
library(tidyr)
library(mlr) #normalization
```

Read the file
```{r lyr}
setwd("C:/Users/Sarasati Palawita/Documents/Personal/BIPM 2017/Master Thesis 2018/Data")
cleaned_data = read_excel("cleaned_data_no_duplicate.xlsx")
```

Data Partition
```{r}
cleaned_data$lyrics <- as.character(cleaned_data$lyrics)
cleaned_data = na.omit(cleaned_data)
cleaned_data <- as_tibble(cleaned_data)
class(cleaned_data)

data <- cleaned_data

data_tf <- cleaned_data %>%
  group_by(genre) %>% 
  nest() %>%            
  mutate(n = c(1250)) %>% 
  mutate(samp = map2(data, n, sample_n)) %>% 
  select(genre, samp) %>%
  unnest()

data <- data_tf
training_data_len <- data_tf %>%
  group_by(genre) %>% 
  nest() %>%            
  mutate(n = c(875)) %>% 
  mutate(samp = map2(data, n, sample_n)) %>% 
  select(genre, samp) %>%
  unnest()

#Make a list of training data song
song_list <- training_data_len$song

test_data_len <- data_tf %>%
  filter(!data_tf$song %in% song_list)

```

To get rid of special characters
```{r removeSpecialChars}
train_source <- training_data_len
test_source <- test_data_len
str(train_source)
# Training data - function to remove special characters
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]", " ", x)
# remove special characters
train_source$lyrics <- sapply(train_source$lyrics, removeSpecialChars)
# convert everything to lower case
train_source$lyrics <- sapply(train_source$lyrics, tolower)

# Test data - function to remove special characters
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]", " ", x)
# remove special characters
test_source$lyrics <- sapply(test_source$lyrics, removeSpecialChars)
# convert everything to lower case
test_source$lyrics <- sapply(test_source$lyrics, tolower)
#lyrics <- lyrics[]

```
To get rid of undesirable words
```{r undesirable_words}
undesirable_words <- c("chorus", "lyrics", "intro", "clichÃfÂ©s",
                       "theres", "bridge", "fe0f",
                       "chorus", "verse", "[chorus]", "[verse]",
                       "2", "2x", "3x", "1x",
                       "4", "ooh", "uurh", "uuh", "pheromone", "poompoom", "3121", 
                       "matic", " ai ", " ca ", " la ", " na ", 
                       " da ", " uh ", " tin ", "  ll", "transcription", "YoncÃfÂ©", "BeyoncÃfÂ©")

```

Count words per line
```{r}
#1. Save the data to excel format for further analysis
data <- data.frame(training_data_len)
write_xlsx(data,"C:/Users/Sarasati Palawita/Documents/Personal/BIPM 2017/Master Thesis 2018/Data/train_data_len.xlsx")
line_sources = read_excel("C:/Users/Sarasati Palawita/Documents/Personal/BIPM 2017/Master Thesis 2018/Data/train_data_len.xlsx")
data <- data.frame(test_data_len)
write_xlsx(data,"C:/Users/Sarasati Palawita/Documents/Personal/BIPM 2017/Master Thesis 2018/Data/test_data_len.xlsx")
test_line_sources = read_excel("C:/Users/Sarasati Palawita/Documents/Personal/BIPM 2017/Master Thesis 2018/Data/test_data_len.xlsx")

#2. Assigned as a tibble data 
class(line_sources)  # it's a tibble (tbl_df)
names(line_sources)
class(test_line_sources)  # it's a tibble (tbl_df)
names(test_line_sources)

#3. Tokenization
tidy_count_wordline <- line_sources %>%
    unnest_tokens(line, lyrics, token = "lines") %>%
    mutate(id = row_number())

tidy_count_word_per_line <- tidy_count_wordline %>%
    unnest_tokens(word, line, token = "words") %>%
    filter(!word %in% undesirable_words) 

test_tidy_count_wordline <- test_line_sources %>%
    unnest_tokens(line, lyrics, token = "lines") %>%
    mutate(id = row_number())

test_tidy_count_word_per_line <- test_tidy_count_wordline %>%
    unnest_tokens(word, line, token = "words") %>%
    filter(!word %in% undesirable_words)

#4. Count total words per line(line = id), n = total words per line
total_count_word_per_line <- tidy_count_word_per_line %>% 
    count(id, song, genre)%>%
    group_by(song)
View(total_count_word_per_line)

test_total_count_word_per_line <- test_tidy_count_word_per_line %>% 
    count(id, song, genre)%>%
    group_by(song)

#5. Calculate the mean value
total_count_words <- total_count_word_per_line %>%
    select(id, song, genre, n) %>%
    group_by(song, genre) %>%
    summarize(mean_per_song = mean(n))

total_mean_words_wordline <- total_count_word_per_line %>%
  select(song, genre, n) %>%
  #filter(song == "00000-million" | song == "1-trillion-dollar")%>%
  group_by(genre) %>%
  summarize(total_mean_genres = mean(n))

total_mean_words_wordline_n <- total_count_word_per_line %>%
  select(song, genre, n, id) %>%
  #filter(song == "00000-million" | song == "1-trillion-dollar")%>%
  group_by(song, genre) %>%
  summarize(total_n = sum(n)) %>%
group_by(song) %>%
summarize(total_mean_n = mean(total_n))

test_total_count_words <- test_total_count_word_per_line %>%
    select(song, genre, n) %>%
    group_by(song, genre) %>%
    summarize(mean_per_song = mean(n))

test_total_mean_words_wordline <- test_total_count_word_per_line %>%
  select(song, genre, n) %>%
  #filter(song == "00000-million" | song == "1-trillion-dollar")%>%
  group_by(genre) %>%
  summarize(total_mean_genres = mean(n))

test_total_mean_words_wordline_n <- test_total_count_word_per_line %>%
  select(song, genre, n) %>%
  #filter(song == "00000-million" | song == "1-trillion-dollar")%>%
  group_by(song, genre) %>%
  summarize(total_n = sum(n)) %>%
group_by(song) %>%
summarize(total_mean_n = mean(total_n))

#6. Merge the dataset
mean_words_per_line <- left_join(total_count_words, total_mean_words_wordline)
mean_words_per_line <- left_join(mean_words_per_line, total_mean_words_wordline_n)
test_mean_words_per_line <- left_join(test_total_count_words, test_total_mean_words_wordline)
test_mean_words_per_line <- left_join(test_mean_words_per_line, test_total_mean_words_wordline_n)

#7. Define the training and testing data
training_count_wordline <- mean_words_per_line
testing_count_wordline <- test_mean_words_per_line

View(training_count_wordline)
#8. Run Cross-Validation
trctrl <- trainControl(method = "repeatedcv", 
                       number = 10, 
                       repeats = 3)
set.seed(123)

#9. Train the model
svm_Linear_words_per_line <- train(genre ~total_mean_genres + mean_per_song, 
                                   data = training_count_wordline,
                                  method = "svmLinear",   
                trControl=trctrl,
                 preProcess = c("center", "scale"),
                 tuneLength = 10)

svm_Linear_words_per_line

#10. Predict the model with testing dataset
test_pred_words_per_line <- predict(svm_Linear_words_per_line, testing_count_wordline)

#11. Check the model accuracy with confusion matrix
mat_count_wordline <- confusionMatrix(table(test_pred_words_per_line, testing_count_wordline$genre))
mat_count_wordline$overall
mat_count_wordline$table

#EXTRA

#Number of words per line
training_count_wordline %>% 
  ggplot() +
  geom_boxplot(aes(genre, mean_per_song), fill = "steelblue", alpha = 0.7) +
  labs(y = "Total words mean per line", x = "Music Genre", 
       title = "Total words per line", 
       subtitle = " ")+
  ylim(0, 50)

word_length <- testing_count_wordline %>% 
  ggplot() +
  geom_boxplot(aes(genre, mean_per_song), fill = "steelblue", alpha = 0.7) +
  labs(y = "Total words mean per line", x = "Music Genre", 
       title = "Total words mean per line", 
       subtitle = " ")+
  ylim(0, 50)


```

Count words per song - 2 - Worked
```{r}
#1. Tokenization
tidy_count_words <- train_source %>%
    unnest_tokens(word, lyrics) %>%
    count(index, song, year, artist, genre, word, sort = TRUE) %>%
   ungroup()

test_tidy_count_words <- test_source %>%
    unnest_tokens(word, lyrics) %>%
    count(index, song, year, artist, genre, word, sort = TRUE) %>%
    ungroup()

#2. Count length
total_count_words <- tidy_count_words %>%
  select(song, genre, n) %>%
  #filter(song == "00000-million" | song == "1-trillion-dollar")%>%
  group_by(song, genre) %>%
  summarize(total_words = sum(n))

test_total_count_words <- test_tidy_count_words %>%
  select(song, genre, n) %>%
  group_by(song, genre) %>%
  summarize(total_words = sum(n))

#3. Calculate mean
total_mean_words <- tidy_count_words %>%
  select(song, genre, n) %>%
  #filter(song == "00000-million" | song == "1-trillion-dollar")%>%
  group_by(song, genre) %>%
  summarize(total_sum = sum(n))%>%
  group_by(song) %>%
  summarize(total_mean_song = mean(total_sum))

total_mean_words_genre <- tidy_count_words %>%
  select(song, genre, n) %>%
  #filter(song == "00000-million" | song == "1-trillion-dollar")%>%
  group_by(song, genre) %>%
  summarize(total_sum = sum(n))%>%
  group_by(genre) %>%
  summarize(total_mean_genre = mean(total_sum))

total_count_mean_words <- left_join(total_count_words, total_mean_words)
total_count_mean_words <- left_join(total_count_mean_words, total_mean_words_genre)

test_total_mean_words <- test_tidy_count_words %>%
  select(song, genre, n) %>%
  #filter(song == "blood-sweat-and-tears")%>%
  group_by(song, genre) %>%
  summarize(total_sum = sum(n))%>%
  group_by(song) %>%
  summarize(total_mean_song = mean(total_sum))

test_total_mean_words_genre <- test_tidy_count_words %>%
  select(song, genre, n) %>%
  #filter(song == "blood-sweat-and-tears")%>%
  group_by(song, genre) %>%
  summarize(total_sum = sum(n))%>%
  group_by(genre) %>%
  summarize(total_mean_genre = mean(total_sum))

test_total_count_mean_words <- left_join(test_total_count_words, test_total_mean_words)
test_total_count_mean_words <- left_join(test_total_count_mean_words, test_total_mean_words_genre)

#4. Define the training and testing data
training_count_words <- total_count_mean_words
testing_count_words <- test_total_count_mean_words
View(training_count_words)

#5. Run Cross-Validation
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

set.seed(123)

#6. Train the model
svm_Linear_words_per_song <- train(genre ~ total_words + total_mean_genre, data = training_count_words, method = "svmLinear",
                 trControl=trctrl,
                 preProcess = c("center", "scale"),
                 tuneLength = 10)

svm_Linear_words_per_song

#7. Predict the model with testing dataset
test_pred_count_words <- predict(svm_Linear_words_per_song, testing_count_words)

#8. Check the model accuracy with confusion matrix
mat_count_words <- confusionMatrix(table(test_pred_count_words, testing_count_words$genre))
mat_count_words$overall
mat_count_words$table

#EXTRA
#Boxplot
total_words <- testing_count_words_div$total_words
total_mean_lex <- testing_count_words_div$total_mean_lex
lexical_diversity <- data.frame(total_mean_lex,total_words)
boxplot(lexical_diversity)

#Number of words per line
word_length <- testing_count_words %>% 
  ggplot() +
  geom_boxplot(aes(genre, total_words), fill = "steelblue", alpha = 0.7) +
  labs(y = "Total words mean per line", x = "Music Genre", 
       title = "Total words per line", 
       subtitle = " ")+
  ylim(0, 1300)


```

Count lines per song - 2 - Worked
```{r}
#1. Save the data to excel format for further analysis
data <- data.frame(training_data_len)
write_xlsx(data,"C:/Users/Sarasati Palawita/Documents/Personal/BIPM 2017/Master Thesis 2018/Data/train_data_len.xlsx")
line_sources = read_excel("C:/Users/Sarasati Palawita/Documents/Personal/BIPM 2017/Master Thesis 2018/Data/train_data_len.xlsx")
data <- data.frame(test_data_len)
write_xlsx(data,"C:/Users/Sarasati Palawita/Documents/Personal/BIPM 2017/Master Thesis 2018/Data/test_data_len.xlsx")
test_line_sources = read_excel("C:/Users/Sarasati Palawita/Documents/Personal/BIPM 2017/Master Thesis 2018/Data/test_data_len.xlsx")

#2. Assigned as a tibble data 
class(line_sources)  # it's a tibble (tbl_df)
names(line_sources)
class(test_line_sources)  # it's a tibble (tbl_df)
names(test_line_sources)

#3. Count lines per song - Tokenization
tidy_line <- line_sources %>%
    unnest_tokens(line, lyrics, token = "lines") %>%
    filter(!line %in% undesirable_words) %>%
    count(index, song, year, artist, genre, line, sort = TRUE) %>%
    ungroup()

test_tidy_line <- test_line_sources %>%
    unnest_tokens(line, lyrics, token = "lines") %>%
    filter(!line %in% undesirable_words) %>%
    count(index, song, year, artist, genre, line, sort = TRUE) %>%
    ungroup()

#4. Count total line per song
total_count_lines <- tidy_line %>%
  select(song, genre, n) %>%
  group_by(song, genre) %>%
  summarize(total_line = sum(n))

test_total_count_lines <- test_tidy_line %>%
  select(song, genre, n) %>%
  #filter(song == "00000-million" | song == "1-trillion-dollar")%>%
  group_by(song, genre) %>%
  summarize(total_line = sum(n))

#5. Calculate total means line per song
total_mean_line_per_song <- tidy_line %>%
  select(song, genre, n) %>%
  group_by(song, genre) %>%
  summarize(total_sum = sum(n))%>%
  group_by(genre) %>%
  summarize(total_mean = mean(total_sum))

total_count_mean_lines <- left_join(total_count_lines, total_mean_line_per_song)

test_total_mean_line_per_song <- test_tidy_line %>%
  select(song, genre, n) %>%
  #filter(song == "00000-million" | song == "1-trillion-dollar")%>%
  group_by(song, genre) %>%
  summarize(total_sum = sum(n))%>%
  group_by(genre) %>%
  summarize(total_mean = mean(total_sum))

test_total_count_mean_lines <- left_join(test_total_count_lines, test_total_mean_line_per_song)

#6. Define the training and testing data
training_count_lines <- total_count_mean_lines
testing_count_lines <- test_total_count_mean_lines

#7. Run Cross-Validation
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

set.seed(123)

#8. Train the model
svm_Linear_lines_per_song <- train(genre ~ total_line + total_mean, data = training_count_lines, method = "svmLinear",
                 trControl=trctrl,
                 preProcess = c("center", "scale"),
                 tuneLength = 10)

svm_Linear_lines_per_song

#9. Predict the model with testing dataset
test_pred_count_lines <- predict(svm_Linear_lines_per_song, testing_count_lines)

#10. Check the model accuracy with confusion matrix
mat_count_lines <- confusionMatrix(table(test_pred_count_lines, testing_count_lines$genre))
mat_count_lines$overall
mat_count_lines$table

#EXTRA
testing_count_lines %>% 
  ggplot() +
  geom_boxplot(aes(genre, total_line), fill = "steelblue", alpha = 0.7) +
  labs(y = "Total line per song", x = "Music Genre", 
       title = "Total line per song", 
       subtitle = " ")+
  ylim(0, 200)

training_count_lines %>% 
  ggplot() +
  geom_boxplot(aes(genre, mean_per_song), fill = "steelblue", alpha = 0.7) +
  labs(y = "Total words mean per line", x = "Music Genre", 
       title = "Total words mean per line", 
       subtitle = " ")+
  ylim(0, 50)

```
